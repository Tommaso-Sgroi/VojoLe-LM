{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install flair tqdm nltk >> /dev/null"
   ],
   "metadata": {
    "id": "3cA_mi97SQzH",
    "ExecuteTime": {
     "end_time": "2025-05-06T18:24:13.378497Z",
     "start_time": "2025-05-06T18:24:09.430738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from flair.data import Sentence\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "nltk.download('punkt_tab')"
   ],
   "metadata": {
    "id": "jCZ7ZTnZX5V0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b3e278cd-4d01-484e-8211-b7f370123c17"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/tommy/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "def load_dictionary(path:str = './gold_dictionary.jsonl'):\n",
    "    import json\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        return [json.loads(js) for js in lines]\n",
    "\n",
    "dictionary = load_dictionary()\n",
    "print(dictionary)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qG9-J9p8Tz5e",
    "outputId": "fef755cb-f817-4823-b5f2-b554a6180153"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sentence embedding"
  },
  {
   "cell_type": "code",
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "# sentences = ['Mi sono pisciato addosso.', 'Mi sono cagato nei pantaloni.']\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('nickprock/sentence-bert-base-italian-xxl-uncased')\n",
    "model = AutoModel.from_pretrained('nickprock/sentence-bert-base-italian-xxl-uncased').to('cpu')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "# model = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2').to('cpu')\n",
    "def embed(sentences: list[str]):\n",
    "  # Tokenize sentences\n",
    "  encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt').to('cpu')\n",
    "\n",
    "  # Compute token embeddings\n",
    "  with torch.no_grad():\n",
    "      model_output = model(**encoded_input)\n",
    "\n",
    "  # Perform pooling. In this case, mean pooling.\n",
    "  sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "  return sentence_embeddings.to('cpu')"
   ],
   "metadata": {
    "id": "XJJ09zt44Gwd",
    "ExecuteTime": {
     "end_time": "2025-05-06T18:59:02.011178Z",
     "start_time": "2025-05-06T18:59:01.124989Z"
    }
   },
   "outputs": [],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "source": [
    "### test\n",
    "embeddings = embed(['che cosa mi sta succedendo?', 'che è?'])\n",
    "F.cosine_similarity(embeddings[0], embeddings[1], dim=0)"
   ],
   "metadata": {
    "id": "bhSZjXZg9OH0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "eb13a615-3784-4349-a6c3-c5b3907e8cd1",
    "ExecuteTime": {
     "end_time": "2025-05-06T18:42:30.507061Z",
     "start_time": "2025-05-06T18:42:30.461872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4948)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "source": [
    "embedding_sentencies_sorianesi = []\n",
    "\n",
    "for i, d in enumerate(dictionary):\n",
    "  sent = ''\n",
    "  key = d['it']\n",
    "  value = d['notes']\n",
    "  if key != '':\n",
    "    sent = key.strip()\n",
    "  elif value != '':\n",
    "    sent = value.strip()\n",
    "  # if key == '':\n",
    "  #   sent = value\n",
    "  # elif value != '':\n",
    "  #   sent = key + ': ' + value\n",
    "  # else:\n",
    "  #   sent = key\n",
    "  embedding_sentencies_sorianesi.append((sent, i))\n",
    "\n",
    "embs = []\n",
    "for x in tqdm([e[0] for e in embedding_sentencies_sorianesi]):\n",
    "  embs.append(embed(x).squeeze(0))\n",
    "\n",
    "embedding_sentencies_sorianesi = torch.stack(embs)\n",
    "embedding_sentencies_sorianesi.shape\n",
    "# embedding_sentencies_sorianesi = torch.tensor(embs)"
   ],
   "metadata": {
    "id": "_1pknkgD6sgI",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1264e5d6-5585-4d11-953e-02c83a41f60b",
    "ExecuteTime": {
     "end_time": "2025-05-06T19:01:02.456167Z",
     "start_time": "2025-05-06T18:59:04.173578Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1820/1820 [01:58<00:00, 15.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1820, 768])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "source": [
    "sentences= [\n",
    "      # \"Ho dato da mangiare ai maiali versando il mangime nel recipiente\",\n",
    "      # \"Ti tiro un pugno fortissimo\",\n",
    "      \"Ho visto Anna andare a funghi con lo zio peppe\"\n",
    "      ]"
   ],
   "metadata": {
    "id": "cZJwnwiD6nvs",
    "ExecuteTime": {
     "end_time": "2025-05-06T19:01:02.551327Z",
     "start_time": "2025-05-06T19:01:02.547573Z"
    }
   },
   "outputs": [],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "source": [
    "Try to get dictionary entities to use in the dataset sentence"
   ],
   "metadata": {
    "id": "9Pz0OoI1_feP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "list(nltk.trigrams(nltk.word_tokenize('ciao come stai?')))"
   ],
   "metadata": {
    "id": "yDlockLBOqIM",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "923cb54d-5b9a-4241-95dc-def8c90778ee",
    "ExecuteTime": {
     "end_time": "2025-05-06T18:28:49.883118Z",
     "start_time": "2025-05-06T18:28:49.501734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ciao', 'come', 'stai'), ('come', 'stai', '?')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "def sentence_similarities(sentence: str, return_scores=False):\n",
    "\n",
    "  sentence_trigram = list(nltk.trigrams(nltk.word_tokenize(sentence)))\n",
    "  sentence_trigram = [' '.join(s) for s in sentence_trigram]\n",
    "\n",
    "  sims = set()\n",
    "  word_sim = {}\n",
    "  for i, sent_emb in enumerate(sentence_trigram):\n",
    "    query_sentence_embeddings = embed(sent_emb)\n",
    "    similarities = F.cosine_similarity(query_sentence_embeddings, embedding_sentencies_sorianesi, dim=1)\n",
    "    \n",
    "    is_similar = (similarities > 0.4)\n",
    "    for i, _is_similar in enumerate(is_similar):\n",
    "        if not _is_similar: continue\n",
    "        \n",
    "        # print(query_sentence_embeddings.shape, sent_emb, similarities[i], dictionary[index])\n",
    "        word = dictionary[i]\n",
    "        word = json.dumps(word)\n",
    "        word_sim[word] = similarities[i]\n",
    "        sims.add(word)\n",
    "  for k,v in word_sim.items():\n",
    "     print(v, k)\n",
    "  return sims\n",
    "\n",
    "return_scores = True\n",
    "for sentence in sentences:\n",
    "  print(sentence)\n",
    "  out = sentence_similarities(sentence, return_scores)\n",
    "\n",
    "  print('-'*200)\n",
    "\n",
    "sentencess = ['Ho visto Anna andare a funghi con lo zio peppe', \"andare\"]\n",
    "sentence_trigram = list(nltk.trigrams(nltk.word_tokenize(sentencess[0])))\n",
    "\n",
    "sentence_trigram = [' '.join(s) for s in sentence_trigram]\n",
    "\n",
    "embeddings = embed(sentence_trigram)\n",
    "embeddings1 = embed(\"andare\")\n",
    "similarities = F.cosine_similarity(embeddings, embeddings1, dim=1)\n",
    "#print(sentences)\n",
    "idx = (similarities > 0.4 ).nonzero()\n",
    "#print(idx.flatten().tolist())\n",
    "similarities"
   ],
   "metadata": {
    "id": "paYzTv4X_fGq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bc1e51fe-8a39-45e7-d47d-d7c0af775ee9",
    "ExecuteTime": {
     "end_time": "2025-05-06T19:01:19.607575Z",
     "start_time": "2025-05-06T19:01:18.921411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ho visto Anna andare a funghi con lo zio peppe\n",
      "tensor(0.4118) {\"it\": \"eccola\", \"dial\": \"estela\", \"notes\": \"\"}\n",
      "tensor(0.4118) {\"it\": \"eccola\", \"dial\": \"ejela\", \"notes\": \"\"}\n",
      "tensor(0.4023) {\"it\": \"bella donna\", \"dial\": \"patacca\", \"notes\": \"\"}\n",
      "tensor(0.4292) {\"it\": \"andana\", \"dial\": \"letta\", \"notes\": \"striscia di terreno coltivabile limitato da due filari di alberi o di piante\"}\n",
      "tensor(0.4552) {\"it\": \"andare\", \"dial\": \"ann\\u00e0\", \"notes\": \"\"}\n",
      "tensor(0.4552) {\"it\": \"andare\", \"dial\": \"gn\\u00e0\", \"notes\": \"\"}\n",
      "tensor(0.4552) {\"it\": \"andare\", \"dial\": \"nn\\u00e0\", \"notes\": \"\"}\n",
      "tensor(0.4552) {\"it\": \"andare\", \"dial\": \"ji\", \"notes\": \"(Tocca ji fo' = Bisogna andare in campagna; pres. io vajo; part. pass. esso gnava; pass. rem. nui gnemma)\"}\n",
      "tensor(0.4552) {\"it\": \"andare\", \"dial\": \"v\\u00e8ne\", \"notes\": \"\"}\n",
      "tensor(0.5325) {\"it\": \"castagne secche\", \"dial\": \"mosciarelle\", \"notes\": \"\"}\n",
      "tensor(0.4305) {\"it\": \"\", \"dial\": \"frangetole\", \"notes\": \"erba selvatica a ciuffi (commestibile)\"}\n",
      "tensor(0.5406) {\"it\": \"fagiolini bianchi\", \"dial\": \"facioli de le curdi\", \"notes\": \"\"}\n",
      "tensor(0.4735) {\"it\": \"fungo\", \"dial\": \"pettinicchia\", \"notes\": \"\"}\n",
      "tensor(0.4561) {\"it\": \"fungo commestibile con cappello di colore bruno\", \"dial\": \"brugnolo\", \"notes\": \"\"}\n",
      "tensor(0.4476) {\"it\": \"germogliare\", \"dial\": \"ciccia\\u2019\", \"notes\": \"\"}\n",
      "tensor(0.4050) {\"it\": \"porcellini di terra\", \"dial\": \"purchitti de sant'antogno\", \"notes\": \"insetti che si appallottolano se impauruti\"}\n",
      "tensor(0.4056) {\"it\": \"fico arva\", \"dial\": \"arva\", \"notes\": \"(tipo di fico)\"}\n",
      "tensor(0.4389) {\"it\": \"\", \"dial\": \"fumogena\", \"notes\": \"malattia degli olivi\"}\n",
      "tensor(0.5427) {\"it\": \"salsiccia\", \"dial\": \"rocchio\", \"notes\": \"\"}\n",
      "tensor(0.6104) {\"it\": \"pomodorini\", \"dial\": \"pummiduritti\", \"notes\": \"piccoli pomodori a grappolo da consumare in inverno\"}\n",
      "tensor(0.4089) {\"it\": \"raspo d'uva\", \"dial\": \"onnaspo\", \"notes\": \"\"}\n",
      "tensor(0.4089) {\"it\": \"raspo d'uva\", \"dial\": \"'nnaspo\", \"notes\": \"\"}\n",
      "tensor(0.4037) {\"it\": \"rovi\", \"dial\": \"r\\u00fbj\", \"notes\": \"\"}\n",
      "tensor(0.4495) {\"it\": \"rovistare\", \"dial\": \"smucina'\", \"notes\": \"\"}\n",
      "tensor(0.4495) {\"it\": \"rovistare\", \"dial\": \"arefucina'\", \"notes\": \"\"}\n",
      "tensor(0.4495) {\"it\": \"rovistare\", \"dial\": \"aremucina'\", \"notes\": \"\"}\n",
      "tensor(0.4316) {\"it\": \"semina\", \"dial\": \"simina\", \"notes\": \"\"}\n",
      "tensor(0.4552) {\"it\": \"\", \"dial\": \"forasacco\", \"notes\": \"spiga selvatica che si trova in mezzo al fieno\"}\n",
      "tensor(0.6153) {\"it\": \"tagliolini\", \"dial\": \"squizzamuso\", \"notes\": \"(in brodo)\"}\n",
      "tensor(0.4026) {\"it\": \"\", \"dial\": \"sarapolli\", \"notes\": \"variet\\u00e0 di erba commestibile\"}\n",
      "tensor(0.4763) {\"it\": \"aglio\", \"dial\": \"ajo\", \"notes\": \"\"}\n",
      "tensor(0.4458) {\"it\": \"carciofi\", \"dial\": \"scarciofuli\", \"notes\": \"\"}\n",
      "tensor(0.4778) {\"it\": \"carciofini\", \"dial\": \"scarciofulitti\", \"notes\": \"\"}\n",
      "tensor(0.4378) {\"it\": \"cavola\", \"dial\": \"cannella\", \"notes\": \"rubinetto per botti\"}\n",
      "tensor(0.4131) {\"it\": \"cime di rapa\", \"dial\": \"marzi\", \"notes\": \"\"}\n",
      "tensor(0.4131) {\"it\": \"cime di rapa\", \"dial\": \"marzitti\", \"notes\": \"\"}\n",
      "tensor(0.5042) {\"it\": \"cipolla\", \"dial\": \"cepa\", \"notes\": \"\"}\n",
      "tensor(0.4722) {\"it\": \"condire\", \"dial\": \"accunn\\u00ec\", \"notes\": \"\"}\n",
      "tensor(0.4767) {\"it\": \"\", \"dial\": \"gravioli\", \"notes\": \"dolci tipici del carnevale ripieni di ricotta condita\"}\n",
      "tensor(0.4614) {\"it\": \"\", \"dial\": \"peparella\", \"notes\": \"erba selvatica commestibile usata come condimento\"}\n",
      "tensor(0.5361) {\"it\": \"fagioli\", \"dial\": \"facioli\", \"notes\": \"\"}\n",
      "tensor(0.4496) {\"it\": \"\", \"dial\": \"sutrina\", \"notes\": \"frittella con acqua, sale e farina\"}\n",
      "tensor(0.5114) {\"it\": \"fagioli giallini\", \"dial\": \"gialluni\", \"notes\": \"genere di fagioli\"}\n",
      "tensor(0.4815) {\"it\": \"pasta asciutta\", \"dial\": \"pastasciucca\", \"notes\": \"\"}\n",
      "tensor(0.4876) {\"it\": \"pasta fatta in casa\", \"dial\": \"lanzagna\", \"notes\": \"\"}\n",
      "tensor(0.4859) {\"it\": \"lasagna\", \"dial\": \"lanzagna\", \"notes\": \"\"}\n",
      "tensor(0.4562) {\"it\": \"\", \"dial\": \"pampariti\", \"notes\": \"piccoli dolcetti fatti in casa\"}\n",
      "tensor(0.4902) {\"it\": \"insaporita\", \"dial\": \"coata\", \"notes\": \"Detto, ad esempio, di una minestra mangiata il giorno dopo la cottura (la magnamo ddimane, intanto se coa; perch\\u00e8 coata j\\u00e8 pi\\u00f9 bbona)\"}\n",
      "tensor(0.4654) {\"it\": \"salsicce di fegato\", \"dial\": \"mazzafeghete\", \"notes\": \"\"}\n",
      "tensor(0.4080) {\"it\": \"\", \"dial\": \"strozzapreti\", \"notes\": \"tipo di pasta fatta in casa (pasta corta, acqua e farina)\"}\n",
      "tensor(0.4644) {\"it\": \"\", \"dial\": \"frascarelli\", \"notes\": \"tipo di pasta fatta in casa\"}\n",
      "tensor(0.4165) {\"it\": \"\", \"dial\": \"scoppolarelle\", \"notes\": \"tipo di susine (le pornella scoppolarelle)\"}\n",
      "tensor(0.4079) {\"it\": \"asparago\", \"dial\": \"spargio\", \"notes\": \"\"}\n",
      "tensor(0.4644) {\"it\": \"condito\", \"dial\": \"accunn\\u00ecto\", \"notes\": \"\"}\n",
      "tensor(0.4353) {\"it\": \"\", \"dial\": \"minturzella\", \"notes\": \"erba aromatica (usata nella cottura della trippa)\"}\n",
      "tensor(0.4165) {\"it\": \"lardo\", \"dial\": \"onto\", \"notes\": \".\"}\n",
      "tensor(0.4165) {\"it\": \"lardo\", \"dial\": \"lonto\", \"notes\": \"\"}\n",
      "tensor(0.4000) {\"it\": \"bollito\", \"dial\": \"allesso\", \"notes\": \"(ill'allesso)\"}\n",
      "tensor(0.4370) {\"it\": \"pigolare\", \"dial\": \"chiocchia\\u2019\", \"notes\": \"(cantare di pennuto)\"}\n",
      "tensor(0.4326) {\"it\": \"pomodoro\", \"dial\": \"pummedoro\", \"notes\": \"\"}\n",
      "tensor(0.4503) {\"it\": \"\", \"dial\": \"cazzimperio\", \"notes\": \"finocchietto condito sale e olio (condimento)\"}\n",
      "tensor(0.4152) {\"it\": \"\", \"dial\": \"fregnaccia\", \"notes\": \"frittata di acqua, sale e farina (sutrina)\"}\n",
      "tensor(0.4707) {\"it\": \"cognato\", \"dial\": \"quinato\", \"notes\": \"\"}\n",
      "tensor(0.4269) {\"it\": \"figlio\", \"dial\": \"fijo\", \"notes\": \"\"}\n",
      "tensor(0.4269) {\"it\": \"figlio\", \"dial\": \"fi'\", \"notes\": \"(curri f\\u00ec = corri ragazzo, dattela a gambe)\"}\n",
      "tensor(0.5076) {\"it\": \"figlioccio\", \"dial\": \"fijano\", \"notes\": \"(di battesimo, cresima)\"}\n",
      "tensor(0.4287) {\"it\": \"padre\", \"dial\": \"pate\", \"notes\": \"\"}\n",
      "tensor(0.4287) {\"it\": \"padre\", \"dial\": \"tata\", \"notes\": \"\"}\n",
      "tensor(0.4124) {\"it\": \"cocciuto\", \"dial\": \"tignoso\", \"notes\": \"\"}\n",
      "tensor(0.6558) {\"it\": \"pedicello\", \"dial\": \"pic\\u00eallo\", \"notes\": \"\"}\n",
      "tensor(0.4666) {\"it\": \"mastello\", \"dial\": \"bronzetto\", \"notes\": \"piccolo mastello in legno di forma ellittica con manico utilizzato in genere per l'asporto di vino dai tini\"}\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2140, 0.3851, 0.4552, 0.3706, 0.2656, 0.2411, 0.1060, 0.1218])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ignore the follwing"
   ],
   "metadata": {
    "id": "gcOGBei6XExO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ignore, just test\n",
    "\n",
    "```\n",
    "# This is formatted as code\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "id": "mWhoFUy969_V"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import re\n",
    "\n",
    "# 1. Carica un modello pre-addestrato adatto all'italiano\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')  # Supporta l'italiano\n",
    "\n",
    "# 2. Funzione per preparare le voci del dizionario\n",
    "def prepare_dictionary_entry(entry):\n",
    "    # Combina i campi per creare un contesto ricco\n",
    "    context = \"\"\n",
    "    if 'it' in entry and entry['it']:\n",
    "        context += entry['it'] + \" \"\n",
    "    if 'dial' in entry and entry['dial']:\n",
    "        context += entry['dial'] + \" \"\n",
    "    if 'notes' in entry and entry['notes']:\n",
    "        context += entry['notes']\n",
    "    return context.strip()\n",
    "\n",
    "# 3. Funzione per generare finestre di contesto dalla frase\n",
    "def generate_context_windows(sentence, window_size=3):\n",
    "    words = re.findall(r'\\w+', sentence.lower())\n",
    "    windows = []\n",
    "\n",
    "    # Genera finestre di parole per preservare il contesto locale\n",
    "    for i in range(len(words)):\n",
    "        start = max(0, i - window_size // 2)\n",
    "        end = min(len(words), i + window_size // 2 + 1)\n",
    "        window = ' '.join(words[start:end])\n",
    "        windows.append(window)\n",
    "\n",
    "    # Aggiungi anche l'intera frase per catturare il contesto globale\n",
    "    windows.append(sentence.lower())\n",
    "\n",
    "    return windows\n",
    "\n",
    "# 4. Funzione principale per trovare voci pertinenti\n",
    "def find_relevant_dictionary_entries(sentence, dictionary, similarity_threshold=0.5):\n",
    "    # Prepara le finestre di contesto dalla frase\n",
    "    context_windows = generate_context_windows(sentence)\n",
    "\n",
    "    # Codifica le finestre di contesto\n",
    "    context_embeddings = model.encode(context_windows, convert_to_tensor=True)\n",
    "\n",
    "    # Prepara le voci del dizionario\n",
    "    dictionary_texts = [prepare_dictionary_entry(entry) for entry in dictionary]\n",
    "\n",
    "    # Codifica le voci del dizionario\n",
    "    dictionary_embeddings = model.encode(dictionary_texts, convert_to_tensor=True)\n",
    "\n",
    "    # Calcola la similarità tra ogni finestra di contesto e ogni voce del dizionario\n",
    "    matches = []\n",
    "    for i, entry in enumerate(dictionary):\n",
    "        # Calcola la massima similarità tra qualsiasi finestra di contesto e questa voce\n",
    "        similarities = util.pytorch_cos_sim(context_embeddings, dictionary_embeddings[i])\n",
    "        max_similarity = float(similarities.max())\n",
    "\n",
    "        if max_similarity >= similarity_threshold:\n",
    "            matches.append({\n",
    "                'entry': entry,\n",
    "                'similarity': max_similarity\n",
    "            })\n",
    "\n",
    "    # Ordina i risultati per similarità decrescente\n",
    "    matches.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "\n",
    "    return matches\n",
    "# La tua frase di esempio\n",
    "sentence = \"Ho visto anna andare a funghi con lo zio peppe\"\n",
    "\n",
    "# Le tue voci di dizionario di esempio\n",
    "dictionary_entries = [\n",
    "    {'it': 'campicello', 'dial': \"fo'\", 'notes': \"(E' ito fò (o de fò) = E' andato in campagna)\"},\n",
    "    {'it': 'ghirlanda', 'dial': 'serta', 'notes': '(detto di agli e cipolle)'},\n",
    "    {'it': 'pendere', 'dial': 'pènna', 'notes': \"(a settembre i'll'uva è fatta e la fico penne) - A PENNA = Penzoloni\"},\n",
    "    {'it': 'qua', 'dial': \"cca'\", 'notes': '(veni a ccà = vieni qua)'},\n",
    "    {'it': 'andare', 'dial': \"anna'\", 'notes': \"(v. anche 'NA - GNA' - JI')\"}\n",
    "]\n",
    "\n",
    "# Trova le voci pertinenti\n",
    "relevant_entries = find_relevant_dictionary_entries(sentence, dictionary_entries, similarity_threshold=0.55)\n",
    "\n",
    "# Mostra i risultati\n",
    "for match in relevant_entries:\n",
    "    print(f\"Similarità: {match['similarity']:.4f} - Voce: {match['entry']}\")\n"
   ],
   "metadata": {
    "id": "azwik_w9NdBi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -U \"huggingface_hub[cli]\" > /dev/null\n",
    "!pip install sentence_transformers~=2.2.2 > /dev/null\n",
    "!huggingface-cli login --token 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"
   ],
   "metadata": {
    "id": "tlD0o-QoXeFw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A try with E5 sentence embedding"
   ],
   "metadata": {
    "id": "Juy-bBaRw8-J"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "def average_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "# Each input text should start with \"query: \" or \"passage: \".\n",
    "# For tasks other than retrieval, you can simply use the \"query: \" prefix.\n",
    "input_texts = [ \"query: how much protein should a female eat\",\n",
    "               \"passage: I shitted my pants.\",\n",
    "               ]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-large-v2', )\n",
    "model = AutoModel.from_pretrained('intfloat/e5-large-v2')\n",
    "\n",
    "# Tokenize the input texts\n",
    "batch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "outputs = model(**batch_dict)\n",
    "embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "\n",
    "# normalize embeddings\n",
    "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "print(embeddings.shape)\n",
    "scores = (embeddings[:1] @ embeddings[1:].T) * 100\n",
    "print(scores.tolist())\n",
    "F.cosine_similarity(embeddings[0], embeddings[1], dim=0)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "n8sCbUutw71X"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Try to use minerva as sentence embedder"
   ],
   "metadata": {
    "id": "CwM1-7onYg_v"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # Get token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('sapienzanlp/Minerva-3B-base-v1.0').to('cuda')\n",
    "model = AutoModel.from_pretrained('sapienzanlp/Minerva-3B-base-v1.0').to('cuda')\n",
    "\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# Sentences for embedding\n",
    "sentences = ['Sei andato a correre al parco', 'Mi sono cagato addosso']\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=False, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Get token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Apply mean pooling to get sentence embeddings\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize the embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "print(sentence_embeddings.shape)\n",
    "cos = F.cosine_similarity(sentence_embeddings[0], sentence_embeddings[1], dim=0)\n",
    "\n",
    "# Output the embeddings\n",
    "print(cos)\n"
   ],
   "metadata": {
    "id": "6ZUAzBfhYgsY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = model.encode([\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "])\n",
    "similarities = model.similarity(embeddings, embeddings)"
   ],
   "metadata": {
    "id": "v18QBpH4cpap"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DIO CANE"
   ],
   "metadata": {
    "id": "QeoJF3moYl3v"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# init embedding\n",
    "embedding = TransformerDocumentEmbeddings('sapienzanlp/Minerva-350M-base-v1.0')\n",
    "\n",
    "# create a sentence\n",
    "sentence1 = Sentence('Sto mangiando una mela'.lower())\n",
    "sentence2 = Sentence('Mi sono cagato addosso'.lower())\n",
    "\n",
    "# embed words in sentence\n",
    "embedding.embed(sentence1)\n",
    "embedding.embed(sentence2)\n",
    "# Due tensori di esempio\n",
    "a = sentence1.embedding\n",
    "b = sentence2.embedding\n",
    "\n",
    "print(a.shape, b.shape)\n",
    "\n",
    "# Calcolo della cosine similarity\n",
    "cos_sim = F.cosine_similarity(a, b, dim=0)\n",
    "\n",
    "print(cos_sim)  # → 1.0"
   ],
   "metadata": {
    "id": "YQkZo5IIb8F1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sentence = Sentence('Ho dato da mangiare ai maiali versando il mangime nel recipiente')\n",
    "\n",
    "embedding.embed(sentence)\n",
    "\n",
    "threshold = 0.85\n",
    "\n",
    "similarities = 0\n",
    "for entry in dictionary:\n",
    "  s = Sentence((entry['it'] + ' ' + entry['notes']).lower())\n",
    "  embedding.embed(s)\n",
    "  cosine = F.cosine_similarity(sentence.embedding, s.embedding, dim=0)\n",
    "  if cosine >= threshold:\n",
    "    similarities += 1\n",
    "    print(cosine, entry)\n",
    "similarities"
   ],
   "metadata": {
    "id": "qNl46or9d7RQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## test a caso"
   ],
   "metadata": {
    "id": "4dWYF35oeQaw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a sentence-transformer model\n",
    "model = SentenceTransformer('microsoft/Multilingual-MiniLM-L12-H384')  # This supports Italian\n",
    "\n",
    "# Create embeddings\n",
    "embedding1 = model.encode('Io e la mia famiglia abbiamo mangiato la frutta dal recipiente')\n",
    "embedding2 = model.encode('MI sono cagato addosso')\n",
    "\n",
    "# Calculate cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "print(similarity)  # This should give a much lower value\n",
    "del model"
   ],
   "metadata": {
    "id": "Pm26I9ZIdMze"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "SDtsrs5ucG6_"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
