{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a40294f4-b17f-47d9-8cd2-6c63cff2e5dc",
   "metadata": {},
   "source": [
    "### Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "id": "dfdbc60a-96bb-4c61-9787-7322fecf7cad",
   "metadata": {},
   "source": [
    "%pip install transformers vllm huggingface-hub ipywidgets"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0567d011-091b-4a21-a9b5-09ee2b037d18",
   "metadata": {},
   "source": [
    "### Login to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "id": "4651b185-66dc-4c85-883a-382d9422b6a2",
   "metadata": {},
   "source": [
    "from huggingface_hub import login\n",
    "login(token='#CHANGE-ME')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "535742d0-884c-4c7a-940d-e28ef98534aa",
   "metadata": {},
   "source": [
    "### Initialize sampling params and LLM"
   ]
  },
  {
   "cell_type": "code",
   "id": "ee162de9-bbcc-4e4d-928e-2426ff1c9c4a",
   "metadata": {},
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_tokens=1000,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1c2bba1f-43c7-4c40-b47c-30ae76f3324d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "llm = LLM(\"CohereLabs/c4ai-command-a-03-2025\", max_model_len=2000, tensor_parallel_size=4, gpu_memory_utilization=0.7) # Set tensor_parallel_size to num of GPUs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "32a6dae1-421a-465d-890e-b95527c3d829",
   "metadata": {},
   "source": [
    "### Start inference"
   ]
  },
  {
   "cell_type": "code",
   "id": "1fdd2bde-a8ed-4a3a-8611-30fbcce35a4a",
   "metadata": {},
   "source": [
    "def print_outputs(outputs):\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Hello! How can I assist you today?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What does \\\"Lo porto io\\\" mean? Is it a bad word to say?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "outputs = llm.chat(conversation,\n",
    "                   sampling_params=sampling_params,\n",
    "                   use_tqdm=True,\n",
    "                   chat_template_content_format=\"string\"\n",
    "                  )\n",
    "print_outputs(outputs)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
