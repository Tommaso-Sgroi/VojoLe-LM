{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": "!pip install tqdm nltk torch transformers flair",
   "metadata": {
    "id": "3cA_mi97SQzH"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "# from flair.embeddings import TransformerWordEmbeddings\n",
    "# from flair.embeddings import TransformerDocumentEmbeddings\n",
    "# from flair.data import Sentence\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "nltk.download('punkt_tab')"
   ],
   "metadata": {
    "id": "jCZ7ZTnZX5V0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b3e278cd-4d01-484e-8211-b7f370123c17"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sentence embedding"
  },
  {
   "cell_type": "code",
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "# sentences = ['Mi sono pisciato addosso.', 'Mi sono cagato nei pantaloni.']\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "# tokenizer = AutoTokenizer.from_pretrained('nickprock/sentence-bert-base-italian-xxl-uncased')\n",
    "# model = AutoModel.from_pretrained('nickprock/sentence-bert-base-italian-xxl-uncased').to('cpu')\n",
    "# ----\n",
    "tokenizer = AutoTokenizer.from_pretrained('sapienzanlp/Minerva-1B-base-v1.0')\n",
    "model = AutoModel.from_pretrained('sapienzanlp/Minerva-1B-base-v1.0').to('cpu')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# ----\n",
    "# tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "# model = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2').to('cpu')\n",
    "\n",
    "def embed(sentences: list[str]):\n",
    "  # Tokenize sentences\n",
    "  encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt').to('cpu')\n",
    "\n",
    "  # Compute token embeddings\n",
    "  with torch.no_grad():\n",
    "      model_output = model(**encoded_input)\n",
    "\n",
    "  # Perform pooling. In this case, mean pooling.\n",
    "  sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "  return sentence_embeddings.to('cpu')"
   ],
   "metadata": {
    "id": "XJJ09zt44Gwd"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "### test\n",
    "embeddings = embed(['che cosa mi sta succedendo?', 'che è?'])\n",
    "F.cosine_similarity(embeddings[0], embeddings[1], dim=0)"
   ],
   "metadata": {
    "id": "bhSZjXZg9OH0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "eb13a615-3784-4349-a6c3-c5b3907e8cd1"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_dictionary(path:str = './gold_dictionary.jsonl'):\n",
    "    import json\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        return [json.loads(js) for js in lines]\n",
    "\n",
    "dictionary = load_dictionary()\n",
    "print(dictionary)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "embedding_sentencies_sorianesi = []\n",
    "\n",
    "for i, d in enumerate(dictionary):\n",
    "  sent = ''\n",
    "  key = d['it']\n",
    "  value = d['notes']\n",
    "  if key != '':\n",
    "    sent = key.strip()\n",
    "  elif value != '':\n",
    "    sent = value.strip()\n",
    "  # if key == '':\n",
    "  #   sent = value\n",
    "  # elif value != '':\n",
    "  #   sent = key + ': ' + value\n",
    "  # else:\n",
    "  #   sent = key\n",
    "  embedding_sentencies_sorianesi.append((sent, i))\n",
    "\n",
    "embs = []\n",
    "for x in tqdm([e[0] for e in embedding_sentencies_sorianesi]):\n",
    "  embs.append(embed(x).squeeze(0))\n",
    "\n",
    "embedding_sentencies_sorianesi = torch.stack(embs)\n",
    "embedding_sentencies_sorianesi.shape\n",
    "# embedding_sentencies_sorianesi = torch.tensor(embs)"
   ],
   "metadata": {
    "id": "_1pknkgD6sgI",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1264e5d6-5585-4d11-953e-02c83a41f60b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "sentences= [\n",
    "      \"Ho dato da mangiare ai maiali versando il mangime nel recipiente\",\n",
    "      \"Ti tiro un pugno fortissimo\",\n",
    "      \"Ho visto Anna andare a funghi con lo zio peppe\",\n",
    "     \"Ho bisogno di fare un pisolino\",\n",
    "    \"Mannaggia la miseria, anche oggi piove\",\n",
    "    \"Maledizione, ho dimenticato il libro di italiano a casa\"\n",
    "      ]"
   ],
   "metadata": {
    "id": "cZJwnwiD6nvs"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Try to get dictionary entities to use in the dataset sentence"
   ],
   "metadata": {
    "id": "9Pz0OoI1_feP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "list(nltk.trigrams(nltk.word_tokenize('ciao come stai?')))"
   ],
   "metadata": {
    "id": "yDlockLBOqIM",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "923cb54d-5b9a-4241-95dc-def8c90778ee"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def sentence_similarities(sentence: str, return_scores=False):\n",
    "\n",
    "  sentence_trigram = list(nltk.trigrams(nltk.word_tokenize(sentence)))\n",
    "  sentence_trigram = [' '.join(s) for s in sentence_trigram]\n",
    "\n",
    "  sims = set()\n",
    "  word_sim = {}\n",
    "  for i, sent_emb in enumerate(sentence_trigram):\n",
    "    query_sentence_embeddings = embed(sent_emb)\n",
    "    similarities = F.cosine_similarity(query_sentence_embeddings, embedding_sentencies_sorianesi, dim=1)\n",
    "    \n",
    "    is_similar = (similarities > 0.6)\n",
    "    for i, _is_similar in enumerate(is_similar):\n",
    "        if not _is_similar: continue\n",
    "        \n",
    "        # print(query_sentence_embeddings.shape, sent_emb, similarities[i], dictionary[index])\n",
    "        word = dictionary[i]\n",
    "        word = json.dumps(word)\n",
    "        word_sim[word] = similarities[i]\n",
    "        sims.add(word)\n",
    "  for k,v in word_sim.items():\n",
    "     print(v, k)\n",
    "  return sims\n",
    "\n",
    "return_scores = True\n",
    "for sentence in sentences:\n",
    "  print(sentence)\n",
    "  out = sentence_similarities(sentence, return_scores)\n",
    "\n",
    "  print('-'*200)\n",
    "\n",
    "sentencess = ['Ho visto Anna andare a funghi con lo zio peppe', \"andare\"]\n",
    "sentence_trigram = list(nltk.trigrams(nltk.word_tokenize(sentencess[0])))\n",
    "\n",
    "sentence_trigram = [' '.join(s) for s in sentence_trigram]\n",
    "\n",
    "embeddings = embed(sentence_trigram)\n",
    "embeddings1 = embed(\"andare\")\n",
    "similarities = F.cosine_similarity(embeddings, embeddings1, dim=1)\n",
    "#print(sentences)\n",
    "idx = (similarities > 0.4 ).nonzero()\n",
    "#print(idx.flatten().tolist())\n",
    "similarities"
   ],
   "metadata": {
    "id": "paYzTv4X_fGq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bc1e51fe-8a39-45e7-d47d-d7c0af775ee9"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ignore the follwing"
   ],
   "metadata": {
    "id": "gcOGBei6XExO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ignore, just test\n",
    "\n",
    "```\n",
    "# This is formatted as code\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "id": "mWhoFUy969_V"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import re\n",
    "\n",
    "# 1. Carica un modello pre-addestrato adatto all'italiano\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')  # Supporta l'italiano\n",
    "\n",
    "# 2. Funzione per preparare le voci del dizionario\n",
    "def prepare_dictionary_entry(entry):\n",
    "    # Combina i campi per creare un contesto ricco\n",
    "    context = \"\"\n",
    "    if 'it' in entry and entry['it']:\n",
    "        context += entry['it'] + \" \"\n",
    "    if 'dial' in entry and entry['dial']:\n",
    "        context += entry['dial'] + \" \"\n",
    "    if 'notes' in entry and entry['notes']:\n",
    "        context += entry['notes']\n",
    "    return context.strip()\n",
    "\n",
    "# 3. Funzione per generare finestre di contesto dalla frase\n",
    "def generate_context_windows(sentence, window_size=3):\n",
    "    words = re.findall(r'\\w+', sentence.lower())\n",
    "    windows = []\n",
    "\n",
    "    # Genera finestre di parole per preservare il contesto locale\n",
    "    for i in range(len(words)):\n",
    "        start = max(0, i - window_size // 2)\n",
    "        end = min(len(words), i + window_size // 2 + 1)\n",
    "        window = ' '.join(words[start:end])\n",
    "        windows.append(window)\n",
    "\n",
    "    # Aggiungi anche l'intera frase per catturare il contesto globale\n",
    "    windows.append(sentence.lower())\n",
    "\n",
    "    return windows\n",
    "\n",
    "# 4. Funzione principale per trovare voci pertinenti\n",
    "def find_relevant_dictionary_entries(sentence, dictionary, similarity_threshold=0.5):\n",
    "    # Prepara le finestre di contesto dalla frase\n",
    "    context_windows = generate_context_windows(sentence)\n",
    "\n",
    "    # Codifica le finestre di contesto\n",
    "    context_embeddings = model.encode(context_windows, convert_to_tensor=True)\n",
    "\n",
    "    # Prepara le voci del dizionario\n",
    "    dictionary_texts = [prepare_dictionary_entry(entry) for entry in dictionary]\n",
    "\n",
    "    # Codifica le voci del dizionario\n",
    "    dictionary_embeddings = model.encode(dictionary_texts, convert_to_tensor=True)\n",
    "\n",
    "    # Calcola la similarità tra ogni finestra di contesto e ogni voce del dizionario\n",
    "    matches = []\n",
    "    for i, entry in enumerate(dictionary):\n",
    "        # Calcola la massima similarità tra qualsiasi finestra di contesto e questa voce\n",
    "        similarities = util.pytorch_cos_sim(context_embeddings, dictionary_embeddings[i])\n",
    "        max_similarity = float(similarities.max())\n",
    "\n",
    "        if max_similarity >= similarity_threshold:\n",
    "            matches.append({\n",
    "                'entry': entry,\n",
    "                'similarity': max_similarity\n",
    "            })\n",
    "\n",
    "    # Ordina i risultati per similarità decrescente\n",
    "    matches.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "\n",
    "    return matches\n",
    "# La tua frase di esempio\n",
    "sentence = \"Ho visto anna andare a funghi con lo zio peppe\"\n",
    "\n",
    "# Le tue voci di dizionario di esempio\n",
    "dictionary_entries = [\n",
    "    {'it': 'campicello', 'dial': \"fo'\", 'notes': \"(E' ito fò (o de fò) = E' andato in campagna)\"},\n",
    "    {'it': 'ghirlanda', 'dial': 'serta', 'notes': '(detto di agli e cipolle)'},\n",
    "    {'it': 'pendere', 'dial': 'pènna', 'notes': \"(a settembre i'll'uva è fatta e la fico penne) - A PENNA = Penzoloni\"},\n",
    "    {'it': 'qua', 'dial': \"cca'\", 'notes': '(veni a ccà = vieni qua)'},\n",
    "    {'it': 'andare', 'dial': \"anna'\", 'notes': \"(v. anche 'NA - GNA' - JI')\"}\n",
    "]\n",
    "\n",
    "# Trova le voci pertinenti\n",
    "relevant_entries = find_relevant_dictionary_entries(sentence, dictionary_entries, similarity_threshold=0.55)\n",
    "\n",
    "# Mostra i risultati\n",
    "for match in relevant_entries:\n",
    "    print(f\"Similarità: {match['similarity']:.4f} - Voce: {match['entry']}\")\n"
   ],
   "metadata": {
    "id": "azwik_w9NdBi"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!export TOKENIZERS_PARALLELISM=false\n",
    "!pip install -U \"huggingface_hub[cli]\" \n",
    "# !pip install sentence_transformers~=2.2.2 "
   ],
   "metadata": {
    "id": "tlD0o-QoXeFw"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!huggingface-cli login --token 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",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A try with E5 sentence embedding"
   ],
   "metadata": {
    "id": "Juy-bBaRw8-J"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "def average_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "# Each input text should start with \"query: \" or \"passage: \".\n",
    "# For tasks other than retrieval, you can simply use the \"query: \" prefix.\n",
    "input_texts = [ \"query: how much protein should a female eat\",\n",
    "               \"passage: I shitted my pants.\",\n",
    "               ]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-large-v2', )\n",
    "model = AutoModel.from_pretrained('intfloat/e5-large-v2')\n",
    "\n",
    "# Tokenize the input texts\n",
    "batch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "outputs = model(**batch_dict)\n",
    "embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "\n",
    "# normalize embeddings\n",
    "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "print(embeddings.shape)\n",
    "scores = (embeddings[:1] @ embeddings[1:].T) * 100\n",
    "print(scores.tolist())\n",
    "F.cosine_similarity(embeddings[0], embeddings[1], dim=0)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "n8sCbUutw71X"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Try to use minerva as sentence embedder"
   ],
   "metadata": {
    "id": "CwM1-7onYg_v"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # Get token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('sapienzanlp/Minerva-3B-base-v1.0').to('cuda')\n",
    "model = AutoModel.from_pretrained('sapienzanlp/Minerva-3B-base-v1.0').to('cuda')\n",
    "\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# Sentences for embedding\n",
    "sentences = ['Sei andato a correre al parco', 'Mi sono cagato addosso']\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=False, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Get token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Apply mean pooling to get sentence embeddings\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize the embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "print(sentence_embeddings.shape)\n",
    "cos = F.cosine_similarity(sentence_embeddings[0], sentence_embeddings[1], dim=0)\n",
    "\n",
    "# Output the embeddings\n",
    "print(cos)\n"
   ],
   "metadata": {
    "id": "6ZUAzBfhYgsY"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = model.encode([\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "])\n",
    "similarities = model.similarity(embeddings, embeddings)"
   ],
   "metadata": {
    "id": "v18QBpH4cpap"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DIO CANE"
   ],
   "metadata": {
    "id": "QeoJF3moYl3v"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# init embedding\n",
    "embedding = TransformerDocumentEmbeddings('sapienzanlp/Minerva-350M-base-v1.0')\n",
    "\n",
    "# create a sentence\n",
    "sentence1 = Sentence('Sto mangiando una mela'.lower())\n",
    "sentence2 = Sentence('Mi sono cagato addosso'.lower())\n",
    "\n",
    "# embed words in sentence\n",
    "embedding.embed(sentence1)\n",
    "embedding.embed(sentence2)\n",
    "# Due tensori di esempio\n",
    "a = sentence1.embedding\n",
    "b = sentence2.embedding\n",
    "\n",
    "print(a.shape, b.shape)\n",
    "\n",
    "# Calcolo della cosine similarity\n",
    "cos_sim = F.cosine_similarity(a, b, dim=0)\n",
    "\n",
    "print(cos_sim)  # → 1.0"
   ],
   "metadata": {
    "id": "YQkZo5IIb8F1"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "sentence = Sentence('Ho dato da mangiare ai maiali versando il mangime nel recipiente')\n",
    "\n",
    "embedding.embed(sentence)\n",
    "\n",
    "threshold = 0.85\n",
    "\n",
    "similarities = 0\n",
    "for entry in dictionary:\n",
    "  s = Sentence((entry['it'] + ' ' + entry['notes']).lower())\n",
    "  embedding.embed(s)\n",
    "  cosine = F.cosine_similarity(sentence.embedding, s.embedding, dim=0)\n",
    "  if cosine >= threshold:\n",
    "    similarities += 1\n",
    "    print(cosine, entry)\n",
    "similarities"
   ],
   "metadata": {
    "id": "qNl46or9d7RQ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## test a caso"
   ],
   "metadata": {
    "id": "4dWYF35oeQaw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a sentence-transformer model\n",
    "model = SentenceTransformer('microsoft/Multilingual-MiniLM-L12-H384')  # This supports Italian\n",
    "\n",
    "# Create embeddings\n",
    "embedding1 = model.encode('Io e la mia famiglia abbiamo mangiato la frutta dal recipiente')\n",
    "embedding2 = model.encode('MI sono cagato addosso')\n",
    "\n",
    "# Calculate cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "print(similarity)  # This should give a much lower value\n",
    "del model"
   ],
   "metadata": {
    "id": "Pm26I9ZIdMze"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "SDtsrs5ucG6_"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
